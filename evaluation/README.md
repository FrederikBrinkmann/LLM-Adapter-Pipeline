# Evaluation Framework

Dieses Modul dient zur systematischen Evaluation verschiedener LLM-Modelle bei der Extraktion strukturierter Daten aus Versicherungs-E-Mails.

## ğŸ“ Struktur

```
evaluation/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ config.py                    # Zentrale Konfiguration
â”œâ”€â”€ metrics.py                   # Bewertungsmetriken
â”œâ”€â”€ runner.py                    # Evaluation durchfÃ¼hren
â”œâ”€â”€ report.py                    # Report generieren
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ datengenerierung/            # Synthetische Testdaten
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ generate_dataset.py
â”‚
â”œâ”€â”€ data/                        # Input-Daten
â”‚   â”œâ”€â”€ synthetic_test_emails.json
â”‚   â””â”€â”€ synthetic_test_emails_gold.json
â”‚
â””â”€â”€ results/                     # Output
    â”œâ”€â”€ evaluation_results.json
    â””â”€â”€ EVALUATION_REPORT.md
```

## ğŸš€ Nutzung

### 1. Synthetische E-Mails generieren (einmalig)

```bash
python -m evaluation.datengenerierung.generate_dataset
```

### 2. Evaluation durchfÃ¼hren

```bash
# Alle Modelle testen
python -m evaluation.runner

# Nur bestimmte Modelle
python -m evaluation.runner --models gpt-4o claude-3-opus

# Quick-Test mit 10 E-Mails
python -m evaluation.runner --limit 10
```

### 3. Report generieren

```bash
python -m evaluation.report
```

## ğŸ“Š Metriken

| Metrik | Beschreibung |
|--------|--------------|
| **Field Accuracy** | % der Felder die exakt mit Gold-Standard Ã¼bereinstimmen |
| **Critical Accuracy** | Accuracy nur fÃ¼r kritische Felder |
| **Schema Valid** | Output entspricht dem erwarteten JSON-Schema |
| **Time (ms)** | Antwortzeit des Modells |

### Kritische Felder

Die folgenden Felder werden bei der Critical Accuracy besonders bewertet:

- `claimant_name` - Name des Antragstellers
- `policy_number` - Versicherungsnummer
- `claim_type` - Art des Schadens
- `incident_date` - Datum des Vorfalls
- `claim_amount` - SchadenshÃ¶he
- `priority` - PrioritÃ¤t des Tickets

### Missing Fields Erkennung

ZusÃ¤tzlich wird bewertet, wie gut das Modell fehlende Felder erkennt:

- **Precision** - Wie viele der erkannten fehlenden Felder sind tatsÃ¤chlich fehlend?
- **Recall** - Wie viele der tatsÃ¤chlich fehlenden Felder wurden erkannt?
- **F1-Score** - Harmonisches Mittel aus Precision und Recall

## ğŸ“„ Output

Der generierte Report (`EVALUATION_REPORT.md`) enthÃ¤lt:

1. **Modellvergleichstabelle** - Alle Modelle sortiert nach Performance
2. **FehlerÃ¼bersicht** - Fehlgeschlagene Tests
3. **Beste/Schlechteste Ergebnisse** - Pro Modell
4. **Metrik-ErklÃ¤rungen**

## ğŸ”§ Konfiguration

Die zentrale Konfiguration befindet sich in `config.py`:

```python
# Timeout fÃ¼r einzelne LLM-Anfragen
DEFAULT_TIMEOUT_SECONDS = 60

# Kritische Felder fÃ¼r Bewertung
CRITICAL_FIELDS = {
    "claimant_name",
    "policy_number",
    "claim_type",
    ...
}

# Felder die beim Vergleich ignoriert werden
IGNORE_FIELDS = {
    "ticket_id",
    "created_timestamp",
    "model_id",
}
```

## ğŸ“Œ NÃ¤chste Schritte

- [ ] Test-Cases mit echten Schadensmeldungen erweitern
- [ ] Prompt Versionen vergleichen (v1, v2, v3)
- [ ] Cost-Analyse hinzufÃ¼gen ($/Accuracy)
- [ ] Kontinuierliche Evaluation (CI/CD Integration)
